{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2809dda5-3777-4191-a890-fd37bd65271d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import paddle\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "\n",
    "from data import load_dict, load_dataset, parse_decodes\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# yapf: disable\n",
    "parser.add_argument(\"--save_dir\", default='./ernie_ckpt', type=str, help=\"The output directory where the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--epochs\", default=5, type=int, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--batch_size\", default=40, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--device\", default=\"gpu\", type=str, choices=[\"cpu\", \"gpu\"] ,help=\"The device to select to train the model, is must be cpu/gpu.\")\n",
    "parser.add_argument(\"--data_dir\", default='./data', type=str, help=\"The folder where the dataset is located.\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75fa9276-1f53-4933-b618-71fe7c863b27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_features(example, tokenizer, label_vocab):\n",
    "    tokens, labels = example\n",
    "    tokenized_input = tokenizer(tokens,\n",
    "                                return_length=True,\n",
    "                                is_split_into_words='token')\n",
    "    # Token '[CLS]' and '[SEP]' will get label 'O'\n",
    "    labels = ['O'] + labels + ['O']\n",
    "    tokenized_input['labels'] = [label_vocab[x] for x in labels]\n",
    "    return tokenized_input['input_ids'], tokenized_input[\n",
    "        'token_type_ids'], tokenized_input['seq_len'], tokenized_input['labels']\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, metric, data_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_pred = 0    \n",
    "    total_gold = 0    \n",
    "    \n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\n",
    "        logits = model(input_ids, seg_ids)\n",
    "        preds = paddle.argmax(logits, axis=-1)\n",
    "        \n",
    "        preds = preds.numpy()\n",
    "        labels = labels.numpy()\n",
    "        lens = lens.numpy()\n",
    "        \n",
    "        for pred, label, length in zip(preds, labels, lens):\n",
    "            pred = pred[:length]\n",
    "            label = label[:length]\n",
    "            \n",
    "            pred_positive = (pred != 0).sum()\n",
    "\n",
    "            gold_positive = (label != 0).sum()\n",
    "            \n",
    "            correct = ((pred == label) & (label != 0)).sum()\n",
    "            \n",
    "            total_correct += correct\n",
    "            total_pred += pred_positive\n",
    "            total_gold += gold_positive\n",
    "    \n",
    "    precision = total_correct / total_pred if total_pred > 0 else 0\n",
    "    recall = total_correct / total_gold if total_gold > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    print(\"[EVAL] Precision: %f - Recall: %f - F1: %f\" % \n",
    "          (precision, recall, f1_score))\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "@paddle.no_grad()\n",
    "def predict(model, data_loader, ds, label_vocab):\n",
    "    all_preds = []\n",
    "    all_lens = []\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\n",
    "        logits = model(input_ids, seg_ids)\n",
    "        preds = paddle.argmax(logits, axis=-1)\n",
    "        # Drop CLS prediction\n",
    "        preds = [pred[1:] for pred in preds.numpy()]\n",
    "        all_preds.append(preds)\n",
    "        all_lens.append(lens)\n",
    "    sentences = [example[0] for example in ds.data]\n",
    "    results = parse_decodes(sentences, all_preds, all_lens, label_vocab)\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(dataset,\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(dataset=dataset,\n",
    "                                batch_sampler=batch_sampler,\n",
    "                                collate_fn=batchify_fn,\n",
    "                                return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04b6e8b-5786-4bc2-b037-bf78ef1798fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_size =512\n",
    "\n",
    "from paddle.nn import GRU\n",
    "from paddle.nn import RNN\n",
    "from paddle.nn import LSTM\n",
    "\n",
    "class BertWithLSTM(paddle.nn.Layer):\n",
    "    def __init__(self, ernie_model, hidden_size, num_classes):\n",
    "        super(ErnieWithLSTM, self).__init__()\n",
    "        self.ernie = ernie_model\n",
    "        self.lstm = LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=1, direction='forward')\n",
    "        self.classifier = paddle.nn.Linear(hidden_size , num_classes)  \n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        ernie_output = self.ernie(input_ids, token_type_ids)\n",
    "        # print(ernie_output)\n",
    "        sequence_output, _ = self.lstm(ernie_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "modelname = 'uer/chinese-roberta-small'\n",
    "\n",
    "paddle.set_device(args.device)\n",
    "rank = paddle.distributed.get_rank()\n",
    "\n",
    "train_ds, dev_ds, test_ds = load_dataset(\n",
    "    datafiles=(os.path.join(args.data_dir, 'train.txt'),\n",
    "                os.path.join(args.data_dir, 'dev.txt'),\n",
    "                os.path.join(args.data_dir, 'test.txt')))\n",
    "\n",
    "label_vocab = load_dict(os.path.join(args.data_dir, 'tag.dic'))\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "trans_func = partial(convert_to_features,\n",
    "                        tokenizer=tokenizer,\n",
    "                        label_vocab=label_vocab)\n",
    "\n",
    "train_ds.map(trans_func)\n",
    "dev_ds.map(trans_func)\n",
    "test_ds.map(trans_func)\n",
    "\n",
    "ignore_label = -1\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int64'\n",
    "        ),  # token_type_ids\n",
    "    Stack(dtype='int64'),  # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label, dtype='int64')  # labels\n",
    "): fn(samples)\n",
    "\n",
    "train_loader = create_dataloader(dataset=train_ds,\n",
    "                                    mode='train',\n",
    "                                    batch_size=args.batch_size,\n",
    "                                    batchify_fn=batchify_fn)\n",
    "\n",
    "dev_loader = create_dataloader(dataset=dev_ds,\n",
    "                                mode='dev',\n",
    "                                batch_size=args.batch_size,\n",
    "                                batchify_fn=batchify_fn)\n",
    "\n",
    "test_loader = create_dataloader(dataset=test_ds,\n",
    "                                mode='test',\n",
    "                                batch_size=args.batch_size,\n",
    "                                batchify_fn=batchify_fn)\n",
    "\n",
    "# Define the model netword and its loss\n",
    "# 1：Directly, only the BERT model\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     modelname, num_classes=len(label_vocab))\n",
    "\n",
    "#2：bert+lst\n",
    "ernie_model = AutoModelForTokenClassification.from_pretrained(modelname, num_classes=hidden_size)\n",
    "model = BertWithLSTM(ernie_model, hidden_size=hidden_size, num_classes=len(label_vocab))\n",
    "\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5,\n",
    "                                    parameters=model.parameters())\n",
    "\n",
    "step = 0\n",
    "args.epochs = 3\n",
    "for epoch in range(args.epochs):\n",
    "    for input_ids, token_type_ids, length, labels in train_loader:\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        step += 1\n",
    "        print(\"[TRAIN] Epoch:%d - Step:%d - Loss: %f\" % (epoch, step, loss))\n",
    "    evaluate(model, metric, dev_loader)\n",
    "\n",
    "if rank == 0:\n",
    "    preds = predict(model, test_loader, test_ds, label_vocab)\n",
    "    file_path = \"ernie_results.txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write(\"\\n\".join(preds))\n",
    "    print(\n",
    "        \"The results have been saved in the file: %s, some examples are shown below: \"\n",
    "        % file_path)\n",
    "    print(\"\\n\".join(preds[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
