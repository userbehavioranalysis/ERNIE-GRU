{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388fd141-555b-44c0-8559-647f1e23e40d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import MapDataset\n",
    "def load_dict(dict_path):\n",
    "    vocab = {}\n",
    "    i = 0\n",
    "    with open(dict_path, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            key = line.strip('\\n')\n",
    "            vocab[key] = i\n",
    "            i += 1\n",
    "    return vocab\n",
    "\n",
    "def load_dataset(datafiles):\n",
    "\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            next(fp)  # Skip header\n",
    "            for line in fp.readlines():\n",
    "                words, labels = line.strip('\\n').split('\\t')\n",
    "                words = words.split('\\002')\n",
    "                labels = labels.split('\\002')\n",
    "                yield words, labels\n",
    "\n",
    "    if isinstance(datafiles, str):\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "def parse_decodes(sentences, predictions, lengths, label_vocab):\n",
    "    \"\"\"Parse the padding result\n",
    "\n",
    "    Args:\n",
    "        sentences (list): the tagging sentences.\n",
    "        predictions (list): the prediction tags.\n",
    "        lengths (list): the valid length of each sentence.\n",
    "        label_vocab (dict): the label vocab.\n",
    "\n",
    "    Returns:\n",
    "        outputs (list): the formatted output.\n",
    "    \"\"\"\n",
    "    predictions = [x for batch in predictions for x in batch]\n",
    "    lengths = [x for batch in lengths for x in batch]\n",
    "    id_label = dict(zip(label_vocab.values(), label_vocab.keys()))\n",
    "\n",
    "    outputs = []\n",
    "    for idx, end in enumerate(lengths):\n",
    "        sent = sentences[idx][:end]\n",
    "        tags = [id_label[x] for x in predictions[idx][:end]]\n",
    "        sent_out = []\n",
    "        tags_out = []\n",
    "        words = \"\"\n",
    "        for s, t in zip(sent, tags):\n",
    "            if t.endswith('-B') or t == 'O':\n",
    "                if len(words):\n",
    "                    sent_out.append(words)\n",
    "                tags_out.append(t.split('-')[0])\n",
    "                words = s\n",
    "            else:\n",
    "                words += s\n",
    "        if len(sent_out) < len(tags_out):\n",
    "            sent_out.append(words)\n",
    "        outputs.append(''.join(\n",
    "            [str((s, t)) for s, t in zip(sent_out, tags_out)]))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2809dda5-3777-4191-a890-fd37bd65271d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import paddle\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# yapf: disable\n",
    "parser.add_argument(\"--save_dir\", default='./ernie_ckpt', type=str, help=\"The output directory where the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--epochs\", default=5, type=int, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--batch_size\", default=40, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--device\", default=\"gpu\", type=str, choices=[\"cpu\", \"gpu\"] ,help=\"The device to select to train the model, is must be cpu/gpu.\")\n",
    "parser.add_argument(\"--data_dir\", default='./data', type=str, help=\"The folder where the dataset is located.\")\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75fa9276-1f53-4933-b618-71fe7c863b27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_features(example, tokenizer, label_vocab):\n",
    "    tokens, labels = example\n",
    "    tokenized_input = tokenizer(tokens,\n",
    "                                return_length=True,\n",
    "                                is_split_into_words='token')\n",
    "\n",
    "    # Token '[CLS]' and '[SEP]' will get label 'O'\n",
    "    labels = ['O'] + labels + ['O']\n",
    "    tokenized_input['labels'] = [label_vocab[x] for x in labels]\n",
    "    return tokenized_input['input_ids'], tokenized_input[\n",
    "        'token_type_ids'], tokenized_input['seq_len'], tokenized_input['labels']\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, metric, data_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_pred = 0    \n",
    "    total_gold = 0    \n",
    "    \n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\n",
    "        logits = model(input_ids, seg_ids)\n",
    "        preds = paddle.argmax(logits, axis=-1)\n",
    "        \n",
    "        preds = preds.numpy()\n",
    "        labels = labels.numpy()\n",
    "        lens = lens.numpy()\n",
    "        \n",
    "        for pred, label, length in zip(preds, labels, lens):\n",
    "            pred = pred[:length]\n",
    "            label = label[:length]\n",
    "            \n",
    "            pred_positive = (pred != 0).sum()\n",
    "\n",
    "            gold_positive = (label != 0).sum()\n",
    "            \n",
    "            correct = ((pred == label) & (label != 0)).sum()\n",
    "            \n",
    "            total_correct += correct\n",
    "            total_pred += pred_positive\n",
    "            total_gold += gold_positive\n",
    "    \n",
    "    precision = total_correct / total_pred if total_pred > 0 else 0\n",
    "    recall = total_correct / total_gold if total_gold > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    print(\"[EVAL] Precision: %f - Recall: %f - F1: %f\" % \n",
    "          (precision, recall, f1_score))\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "@paddle.no_grad()\n",
    "def predict(model, data_loader, ds, label_vocab):\n",
    "    all_preds = []\n",
    "    all_lens = []\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\n",
    "        logits = model(input_ids, seg_ids)\n",
    "        preds = paddle.argmax(logits, axis=-1)\n",
    "        # Drop CLS prediction\n",
    "        preds = [pred[1:] for pred in preds.numpy()]\n",
    "        all_preds.append(preds)\n",
    "        all_lens.append(lens)\n",
    "    sentences = [example[0] for example in ds.data]\n",
    "    results = parse_decodes(sentences, all_preds, all_lens, label_vocab)\n",
    "    return results\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(dataset,\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(dataset=dataset,\n",
    "                                batch_sampler=batch_sampler,\n",
    "                                collate_fn=batchify_fn,\n",
    "                                return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994e1dbd-a7c1-499a-aa1f-f90e56844702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_size =256\n",
    "from paddle.nn import GRU\n",
    "from paddle.nn import RNN\n",
    "from paddle.nn import LSTM\n",
    "\n",
    "class ErnieWithLSTM(paddle.nn.Layer):\n",
    "    def __init__(self, ernie_model, hidden_size, num_classes):\n",
    "        super(ErnieWithLSTM, self).__init__()\n",
    "        self.ernie = ernie_model\n",
    "        self.lstm = GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=1, direction='forward')\n",
    "        self.classifier = paddle.nn.Linear(hidden_size , num_classes)  #Bidirectional here requires:hidden_size*2\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        ernie_output = self.ernie(input_ids, token_type_ids)\n",
    "        # print(ernie_output)\n",
    "        sequence_output, _ = self.lstm(ernie_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "modelname = 'ernie-3.0-medium-zh'\n",
    "\n",
    "paddle.set_device(args.device)\n",
    "rank = paddle.distributed.get_rank()\n",
    "\n",
    "train_ds, dev_ds, test_ds = load_dataset(\n",
    "    datafiles=(os.path.join(args.data_dir, 'train.txt'),\n",
    "                os.path.join(args.data_dir, 'dev.txt'),\n",
    "                os.path.join(args.data_dir, 'test.txt')))\n",
    "\n",
    "label_vocab = load_dict(os.path.join(args.data_dir, 'tag.dic'))\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "trans_func = partial(convert_to_features,\n",
    "                        tokenizer=tokenizer,\n",
    "                        label_vocab=label_vocab)\n",
    "\n",
    "train_ds.map(trans_func)\n",
    "dev_ds.map(trans_func)\n",
    "test_ds.map(trans_func)\n",
    "\n",
    "ignore_label = -1\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int64'\n",
    "        ),  # token_type_ids\n",
    "    Stack(dtype='int64'),  # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label, dtype='int64')  # labels\n",
    "): fn(samples)\n",
    "\n",
    "train_loader = create_dataloader(dataset=train_ds,\n",
    "                                    mode='train',\n",
    "                                    batch_size=args.batch_size,\n",
    "                                    batchify_fn=batchify_fn)\n",
    "\n",
    "dev_loader = create_dataloader(dataset=dev_ds,\n",
    "                                mode='dev',\n",
    "                                batch_size=args.batch_size,\n",
    "                                batchify_fn=batchify_fn)\n",
    "\n",
    "test_loader = create_dataloader(dataset=test_ds,\n",
    "                                mode='test',\n",
    "                                batch_size=args.batch_size,\n",
    "                                batchify_fn=batchify_fn)\n",
    "\n",
    "# Define the model netword and its loss\n",
    "# Method 1: Directly Utilizing Only the ERNIE Model\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     modelname, num_classes=len(label_vocab))\n",
    "\n",
    "# Method Two: Ernie Model + LSTM Model\n",
    "ernie_model = AutoModelForTokenClassification.from_pretrained(modelname, num_classes=hidden_size)\n",
    "model = ErnieWithLSTM(ernie_model, hidden_size=hidden_size, num_classes=len(label_vocab))\n",
    "\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5,\n",
    "                                    parameters=model.parameters())\n",
    "\n",
    "step = 0\n",
    "for epoch in range(args.epochs):\n",
    "    for input_ids, token_type_ids, length, labels in train_loader:\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        step += 1\n",
    "        print(\"[TRAIN] Epoch:%d - Step:%d - Loss: %f\" % (epoch, step, loss))\n",
    "    evaluate(model, metric, dev_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2acfd6ef-72d5-483b-8db2-e8ded12407ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print('Initiating predictive data analysisï¼š')\n",
    "\n",
    "if rank == 0:\n",
    "    prediction_times = []\n",
    "    for i in range(10):\n",
    "        print(f\"The {i+1}/10 prediction...\")\n",
    "        start_time = time.time()\n",
    "        preds = predict(model, test_loader, test_ds, label_vocab)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        prediction_times.append(elapsed_time)\n",
    "        print(f\"The {i+1} prediction: {elapsed_time:.4f} s\")\n",
    "    \n",
    "    average_time = sum(prediction_times) / len(prediction_times)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Predictive Time Statistics:\")\n",
    "    for i, t in enumerate(prediction_times):\n",
    "        print(f\"The {i+1} prediction: {t:.4f} s\")\n",
    "    print(f\"\\nAverage Predictive Time: {average_time:.4f} s\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47593d9-20b5-478f-822f-f6a5d89bec20",
   "metadata": {},
   "source": [
    "ernie  0.24s\n",
    "\n",
    "bert  0.21s\n",
    "\n",
    "ernie+gru 0.38s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f2896f2-adf2-4149-a9ae-1081ba2b1154",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "if rank == 0:\n",
    "    preds = predict(model, test_loader, test_ds, label_vocab)\n",
    "    file_path = \"ernie_results.txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write(\"\\n\".join(preds))\n",
    "    # Print some examples\n",
    "    print(\n",
    "        \"The results have been saved in the file: %s, some examples are shown below: \"\n",
    "        % file_path)\n",
    "    print(\"\\n\".join(preds[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
